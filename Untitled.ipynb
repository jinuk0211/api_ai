{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a92c6a6-b44a-48e4-9906-177dca2c7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "# prompt: 현재 path\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597f0bb6-93f5-4727-881f-2953aec2c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.0.post2.tar.gz (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu118)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.0.post2-cp310-cp310-linux_x86_64.whl size=188946477 sha256=e0ff1d33d44a2d9b93b672a92a1663329439d14e259fa60068589d1d7202b078\n",
      "  Stored in directory: /root/.cache/pip/wheels/bf/e3/ed/5e845387d52f2debd1bafb847bf3d774d3f0a3c8e31b1dc948\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.0.post2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.1)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install qwen-vl-utils -q\n",
    "!pip install transformers==4.45.2 -q\n",
    "!pip install -U accelerate\n",
    "!pip install opencv-python\n",
    "!pip install huggingface_hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd3b757-df34-4995-a9a0-64315cca8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: tensor([0.8772])\n",
      "Sim: tensor([0.7552])\n",
      "Sim: tensor([0.8453])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: tensor([0.5183])\n",
      "Sim: tensor([0.6826])\n",
      "Sim: tensor([0.7046])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: tensor([0.7860])\n",
      "Sim: tensor([0.7698])\n",
      "Sim: tensor([0.9880])\n",
      "Sim: tensor([0.8085])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n",
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim: tensor([0.7922])\n",
      "Sim: tensor([0.7568])\n",
      "Sim: tensor([0.2840])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "import math\n",
    "from transformers import CLIPFeatureExtractor,CLIPVisionModel\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_path = 'openai/clip-vit-large-patch14-336'\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(model_path)\n",
    "vision_tower = CLIPVisionModel.from_pretrained(model_path).cuda()\n",
    "vision_tower.requires_grad_(False)\n",
    "\n",
    "\n",
    "def get_resized_wh(width, height, max_size):\n",
    "    if width > max_size or height > max_size:\n",
    "        if width > height:\n",
    "            new_width = max_size\n",
    "            new_height = int(height * (max_size / width))\n",
    "        else:\n",
    "            new_height = max_size\n",
    "            new_width = int(width * (max_size / height))\n",
    "    else:\n",
    "        new_width = width\n",
    "        new_height = height\n",
    "    return new_width, new_height\n",
    "\n",
    "def check_pure(mtx):\n",
    "    unique_elements = np.unique(mtx)\n",
    "    return len(unique_elements) == 1\n",
    "\n",
    "def extract_second(image_filename):\n",
    "    return image_filename.split('/')[-1].replace('.png', '').split('_')[-1]\n",
    "\n",
    "def calculate_clip_feature_sim_2(image_1, image_2):\n",
    "    input_1 = feature_extractor(images=image_1, return_tensors=\"pt\", padding=True)\n",
    "    input_2 = feature_extractor(images=image_2, return_tensors=\"pt\", padding=True)\n",
    "    image_feature_1 = vision_tower(**input_1.to(device=vision_tower.device), output_hidden_states=True).hidden_states[-1][:, 0]\n",
    "    image_feature_2 = vision_tower(**input_2.to(device=vision_tower.device), output_hidden_states=True).hidden_states[-1][:, 0]\n",
    "    similarity = F.cosine_similarity(image_feature_1.to(device='cpu'), image_feature_2.to(device='cpu'), dim=1)\n",
    "    print(f'Sim: {similarity}')\n",
    "    return similarity\n",
    "\n",
    "def frame_interval_file(video_path, keyframe_interval, shortest_duration, longest_duration, window_threshold, output_dir):\n",
    "    # if 전체 비디오시\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = math.ceil(video_fps * keyframe_interval)\n",
    "    frame_list = []\n",
    "    cnt_tmp = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            #fps 60, 100초짜리 영상이라 할때 6000/60 > 100 이면 비디오 길이를 넘어간다\n",
    "        if cnt_tmp / video_fps > longest_duration:\n",
    "            break\n",
    "        if cnt_tmp == 0 and check_pure(frame) == True:\n",
    "            pure_cnt = 1\n",
    "            while pure_cnt < frame_interval:\n",
    "                ret, frame = cap.read()\n",
    "                if check_pure(frame) != True:\n",
    "                    break\n",
    "                pure_cnt += 1\n",
    "        frame_list.append(frame)\n",
    "        cnt_tmp += 1\n",
    "    if len(frame_list) > math.ceil(video_fps * shortest_duration):\n",
    "        start_frame_idx = 0\n",
    "        selected_frame_list = [0]\n",
    "        if len(frame_list) > frame_interval:\n",
    "            for i in range(1, len(frame_list)):\n",
    "                if i % frame_interval == 0:\n",
    "                    dynamic_sim = calculate_clip_feature_sim_2(frame_list[start_frame_idx], frame_list[i])\n",
    "                    if dynamic_sim < window_threshold:\n",
    "                        selected_frame_list.append(i)\n",
    "                        start_frame_idx = i\n",
    "        if len(selected_frame_list) == 1:\n",
    "            selected_frame_list.append(len(frame_list)-1)\n",
    "        elif (len(frame_list)-selected_frame_list[-1]) >= frame_interval:\n",
    "            selected_frame_list.append(len(frame_list)-1)\n",
    "        for fc in selected_frame_list:\n",
    "            current_time = fc / video_fps\n",
    "            time_str = f\"{current_time:04.2f}\"\n",
    "            frame_filename = f\"frame_{time_str}.png\"\n",
    "            frame_filename = os.path.join(output_dir, frame_filename)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            new_width, new_height = get_resized_wh(width, height, 1024)\n",
    "            if new_width == width and new_height == height:\n",
    "                pass\n",
    "            else:\n",
    "                frame_list[fc] = cv2.resize(frame_list[fc], (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "            suc = cv2.imwrite(frame_filename, frame_list[fc])\n",
    "            if not suc:\n",
    "                print(f\"Failed to save frame {time_str} to {frame_filename}.\")\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# frame_interval_file('file:///2024_11_30 13_51.mp4',2,10,120,0.8,'workspace/keyframe')\n",
    "video_path = '/workspace/2024_11_30 13_51.mp4'\n",
    "keyframe_interval = 2\n",
    "shortest_duration = 10\n",
    "longest_duration = 120\n",
    "window_threshold = 0.8\n",
    "output_dir = 'keyframe'\n",
    "# save keyframes to 'output_dir'\n",
    "frame_interval_file(video_path, keyframe_interval, shortest_duration, longest_duration, window_threshold,\n",
    "output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fca187e-81be-4f76-a315-b89a8a77304f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2a16e5e6ec437f9e749b62ea2e3908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4303d98c8b41eda47dd5ed0b161f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185ded9ffc694e3e9f45694e1e8dc716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f699a9c35384de8ac5f6125810851dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1979c7573dff42a3b82599a50b119883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e52d3984c9420bb25ec54cb40ea268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87b875d08044189de6c7f96102f56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550fbde86ad14761ba6a1dffc12e540f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd58ba56824b432cb09877be7a22679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3e311db6f446f9826bd6a3688382d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e6b81becd7411d8d54aa2693d4826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a3ff0c9e2b4c83a453574e7f6c02b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4060b69b9db45e4b1c174176e7f3725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3412157dd74f0f92fbc32a62099d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4db59f8adf74decaa92b1299101cf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c672a9c57e744257a660382621fa659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "# # default: Load the model on the available device(s)\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"cuda\"\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6990d252-76a2-4cdf-8c4a-f0e2cd8cc986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image appears to be a close-up of a textured surface. The texture seems to be composed of fine, parallel lines that create a striped pattern. The lines are consistent in width and spacing, giving the surface a uniform appearance. The color of the surface is a light, neutral tone, possibly a shade of gray or white. The lighting in the image is soft, which helps to highlight the texture without creating harsh shadows or reflections. The overall impression is one of smoothness and uniformity, with the texture providing a subtle visual interest.']\n",
      "['The image depicts a person standing in a room. The individual is wearing a camouflage-patterned shirt with the letters \"R.O.K.A.\" on the chest and black underwear. The room has a light-colored wall and a ceiling with a visible air conditioning unit. There is a bed with a yellow and green patterned bedspread in the background, and a wooden bookshelf with various items on it. The room appears to be well-lit, likely from natural light.']\n",
      "['The image depicts a person in a room, seemingly in the process of moving or adjusting something on a bed. The individual is wearing a camouflage-patterned shirt and dark shorts. The bed has a wooden frame and is covered with a patterned bedspread. On the bed, there are some clothes, including a green shirt and a pair of shorts. \\n\\nIn the background, there is a wall-mounted air conditioning unit, which is white and rectangular. The wall is painted in a light blue color. To the left of the air conditioning unit, there is a wooden bookshelf with several shelves, some of which are empty, while others have']\n",
      "['The image shows a person lying on a bed in a room. The person is wearing a camouflage-patterned shirt and black shorts. The bed has a yellow bedspread with a floral pattern. The room has a light blue wall and a white air conditioning unit mounted on the wall above the bed. There is a wooden bookshelf to the left of the bed, and a white electrical outlet with a plug connected to it. The room appears to be well-lit, possibly by natural light coming from a window outside the frame.']\n",
      "['The image depicts a person in a room, seemingly in the process of getting dressed or changing clothes. The individual is bending over a bed, reaching for clothing items. The room has a simple decor, with a bed that has a wooden frame and a mattress. On the bed, there are some clothes, including a green shirt and other items. The person is wearing a camouflage-patterned shirt and black shorts. \\n\\nIn the background, there is a bookshelf with several books and items on it. The wall behind the bed is painted in a light color, and there is an air conditioning unit mounted on the wall above the bed. The']\n",
      "['The image depicts a room with a bed in the foreground. The bed has a wooden frame and is covered with a yellow and green patterned bedspread. On the bed, there are some clothes, including a green shirt and a pair of pants. To the left of the bed, there is a wooden bookshelf with several shelves, some of which are empty, and others contain various items. The wall behind the bed is painted in a light blue color, and there is a white air conditioning unit mounted on the wall above the bed. The ceiling is white, and there is a light fixture on the ceiling. The room appears to be']\n",
      "['The image depicts a room with a few notable features. The room has a light blue wall and a white ceiling. On the left side of the image, there is a wooden bookshelf with several shelves, some of which are empty, while others contain books or other items. The bookshelf is positioned against the wall.\\n\\nIn the center of the image, there is a wall-mounted air conditioning unit. Below the air conditioning unit, there is a white electrical outlet on the wall. A white cord is plugged into the outlet, extending down and connecting to a device or appliance that is not visible in the image.\\n\\nOn the right side of the']\n",
      "['The image depicts a person standing in a room. The individual is wearing a camouflage-patterned t-shirt and black shorts. They are barefoot and appear to be walking or standing in a relaxed manner. The room has a bed with a wooden headboard and a mattress covered with a yellow sheet. On the bed, there are some clothes, including a green shirt and a pair of white sneakers. The room also features a wooden bookshelf with several books and items on it. The wall behind the person is painted in a light color, and there is an air conditioning unit mounted on the wall. The ceiling is white, and there is a']\n",
      "['The image depicts a small, neatly arranged bedroom. The room features a light blue wall and a wooden bed frame. The bed is made with a yellow and green patterned bedspread. On the bed, there are some clothes, including a green shirt and a pair of pants. \\n\\nTo the left of the bed, there is a wooden bookshelf with several shelves, some of which are empty, while others contain various items. The bookshelf is positioned against the wall, which is painted in a light blue color. \\n\\nAbove the bed, there is a white air conditioning unit mounted on the wall. The air conditioning unit is connected to']\n",
      "['The image depicts a small, neatly organized bedroom. The room features a bunk bed with a wooden frame. The upper bunk is neatly made with a yellow bedspread and a white pillow. The lower bunk is also made, with a gray bedspread and a white pillow. \\n\\nOn the wall above the bunk bed, there is an air conditioning unit installed. The wall is painted in a light gray color, and there is a white electrical outlet on the wall near the air conditioning unit. \\n\\nTo the right side of the image, there is a hanging rack with some clothes hanging on it. The clothes include a white shirt and a blue shirt']\n"
     ]
    }
   ],
   "source": [
    "txt_list = []\n",
    "frame_list=[]\n",
    "for i in os.listdir('keyframe'):\n",
    "  frame_time = i.split('_')[1].split('.')[0]\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\n",
    "                  \"type\": \"image\",\n",
    "                  \"image\": f\"keyframe/{i}\",\n",
    "              },\n",
    "              {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "          ],\n",
    "      }\n",
    "  ]\n",
    "\n",
    "  #  inference\n",
    "  text = processor.apply_chat_template(\n",
    "      messages, tokenize=False, add_generation_prompt=True\n",
    "  )\n",
    "  image_inputs, video_inputs = process_vision_info(messages)\n",
    "  inputs = processor(\n",
    "      text=[text],\n",
    "      images=image_inputs,\n",
    "      videos=video_inputs,\n",
    "      padding=True,\n",
    "      return_tensors=\"pt\",\n",
    "  )\n",
    "  inputs = inputs.to(\"cuda\")\n",
    "\n",
    "  # Inference\n",
    "  generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "  generated_ids_trimmed = [\n",
    "      out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "  output_text = processor.batch_decode(\n",
    "      generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "  )\n",
    "  print(output_text)\n",
    "  txt_list.append(output_text)\n",
    "  frame_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3b6b66-d89d-4bc3-9bbc-a910c20e2249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frame_26.00.png',\n",
       " 'frame_24.00.png',\n",
       " 'frame_22.00.png',\n",
       " 'frame_16.00.png',\n",
       " 'frame_14.00.png',\n",
       " 'frame_12.00.png',\n",
       " 'frame_10.00.png',\n",
       " 'frame_8.00.png',\n",
       " 'frame_4.00.png',\n",
       " 'frame_0.00.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52ccf530-7234-4b5e-bb62-643b279bfe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ee3e0e25dd4f85bdeccfcec1deea91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca05036b50d454a99d5a21baef1876f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e93f7eaf593415a86ce6ed705b93a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 4965.80 MB. The target location /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/blobs only has 1832.73 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741b0bf6991b4e4f8f6271c95971716f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a337180d2c4a4463af697917769dedd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 4965.80 MB. The target location /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/blobs only has 3.95 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2adae07752414eb01dcc4b4bedf59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  37%|###6      | 1.83G/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model meta-llama/Llama-3.2-3B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m login(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_ateIxCZeCvUGCxWNBLoqfhIsRLwDgBqRqC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:301\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    300\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model meta-llama/Llama-3.2-3B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face에서 받은 토큰을 사용하여 로그인\n",
    "login('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')\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.int8,\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd49480-b925-498c-a0ce-99cd3351fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for txt, frame in zip(txt_list, frame_list):\n",
    "  message = f'{txt} : {frame}'\n",
    "  # item_data = {\n",
    "  #   \"name\": frame,  # Use frame name as item\n",
    "  #   \"description\": txt[0], \n",
    "  #   \"price\": 10.0,\n",
    "  #   \"quantity\": 1\n",
    "  # }\n",
    "log_list = [f'{frame_list[i]}: {txt_list[i]}' for i in range(len(frame_list))]\n",
    "\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are chatbot who should summarize frame time and corresponding caption text!\"},\n",
    "{\"role\": \"user\", \"content\": f\"you have to summarize {','.join(log_list)}\"},]\n",
    "\n",
    "\n",
    "outputs = pipe(\n",
    "messages,\n",
    "max_new_tokens=256,)\n",
    "\n",
    "\n",
    "    # return {\"message\":','.join(log_list)},\n",
    "    #         \"sumamry\": outputs}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
